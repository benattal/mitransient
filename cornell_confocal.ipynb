{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scalar_rgb', 'scalar_spectral', 'scalar_spectral_polarized', 'llvm_ad_rgb', 'llvm_ad_mono', 'llvm_ad_mono_polarized', 'llvm_ad_spectral', 'llvm_ad_spectral_polarized', 'cuda_ad_rgb', 'cuda_ad_mono', 'cuda_ad_mono_polarized', 'cuda_ad_spectral', 'cuda_ad_spectral_polarized']\n",
      "Using mitsuba version: 3.7.0\n",
      "Using mitransient version: 1.2.0\n"
     ]
    }
   ],
   "source": [
    "# If you have compiled Mitsuba 3 yourself, you will need to specify the path\n",
    "# to the compilation folder\n",
    "# import sys\n",
    "# sys.path.insert(0, '<mitsuba-path>/mitsuba3/build/python')\n",
    "import mitsuba as mi\n",
    "# To set a variant, you need to have set it in the mitsuba.conf file\n",
    "# https://mitsuba.readthedocs.io/en/latest/src/key_topics/variants.html\n",
    "print(mi.variants())\n",
    "mi.set_variant('cuda_ad_rgb')\n",
    "\n",
    "import mitransient as mitr\n",
    "\n",
    "print('Using mitsuba version:', mi.__version__)\n",
    "print('Using mitransient version:', mitr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import drjit as dr\n",
    "import numpy as np\n",
    "\n",
    "def get_rays_mitsuba(H, W, fx, fy, cx, cy, c2w_matrix):\n",
    "    \"\"\"\n",
    "    Generate camera rays using DrJit/Mitsuba instead of PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        H, W: Image height and width\n",
    "        fx, fy: Focal lengths\n",
    "        cx, cy: Principal point coordinates\n",
    "        c2w_matrix: Camera-to-world transformation matrix (4x4 numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        origins: Ray origins (H, W, 3) numpy array\n",
    "        viewdirs: Ray directions (H, W, 3) numpy array\n",
    "    \"\"\"\n",
    "    # Create pixel coordinates\n",
    "    num_pixels = H * W\n",
    "    idx = np.arange(num_pixels)\n",
    "    x = idx % W\n",
    "    y = idx // W\n",
    "    \n",
    "    # Convert to image coordinates with pixel centers\n",
    "    # Using the same convention as the torch code: (x - cx + 0.5) / fx\n",
    "    pixel_x = (x - cx + 0.5) / fx\n",
    "    pixel_y = (y - cy + 0.5) / fy * -1.0  # Flip Y axis\n",
    "    \n",
    "    # Create direction vectors in camera space\n",
    "    # Format: [pixel_x, pixel_y, -1.0] (pointing into the scene)\n",
    "    dirs_camera = np.stack([pixel_x, pixel_y, np.full(num_pixels, -1.0)], axis=-1)\n",
    "    \n",
    "    # Extract rotation and translation from c2w matrix\n",
    "    rotation = c2w_matrix[:3, :3]\n",
    "    translation = c2w_matrix[:3, 3]\n",
    "    \n",
    "    # Transform directions to world space\n",
    "    directions = dirs_camera @ rotation.T  # (num_pixels, 3) @ (3, 3).T\n",
    "    \n",
    "    # Normalize directions\n",
    "    directions = directions / np.linalg.norm(directions, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Broadcast camera origin to all rays\n",
    "    origins = np.broadcast_to(translation, (num_pixels, 3))\n",
    "    \n",
    "    # Reshape to image dimensions\n",
    "    origins = origins.reshape(H, W, 3)\n",
    "    viewdirs = directions.reshape(H, W, 3)\n",
    "    \n",
    "    # Negate viewdirs to match torch convention\n",
    "    viewdirs = -1 * viewdirs\n",
    "    \n",
    "    return origins, viewdirs\n",
    "\n",
    "d = mitr.cornell_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origins shape: (255, 255, 3)\n",
      "Rays shape: (255, 255, 3)\n",
      "Sample ray direction at (0,0): [-0.31976103 -0.31976101 -0.89191131]\n",
      "Sample ray origin at (0,0): [0.  0.  3.9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Image dimensions\n",
    "H, W = 255, 255\n",
    "fov = 0.69097585\n",
    "\n",
    "# Compute camera intrinsics\n",
    "fx = (W / 2.0) / np.tan(fov / 2.0)\n",
    "fy = fx\n",
    "cx = float(W) / 2.0\n",
    "cy = float(H) / 2.0\n",
    "\n",
    "# Get camera-to-world matrix from the sensor\n",
    "c2w_transform = d['sensor']['to_world']\n",
    "c2w_matrix = np.array(c2w_transform.matrix)\n",
    "\n",
    "# Generate rays using Mitsuba/DrJit approach\n",
    "origins, rays = get_rays_mitsuba(H, W, fx, fy, cx, cy, c2w_matrix)\n",
    "\n",
    "print(f\"Origins shape: {origins.shape}\")\n",
    "print(f\"Rays shape: {rays.shape}\")\n",
    "print(f\"Sample ray direction at (0,0): {rays[0, 0]}\")\n",
    "print(f\"Sample ray origin at (0,0): {origins[0, 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "projector-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parametric projector configuration:\n",
      "  Grid size: 1 rows x 3 cols = 3 spots\n",
      "  Gaussian sigma (normalized): 0.001\n",
      "  Intensity per spot: 33333.333333333336 (base: 100000.0)\n",
      "  Samples per pixel: 30000 (base: 10000)\n",
      "  Max rejection samples: 4\n"
     ]
    }
   ],
   "source": [
    "# Create parametric projector with Gaussian spots using the new ConfocalProjector emitter\n",
    "import numpy as np\n",
    "\n",
    "# ==== CONFIGURABLE PARAMETERS ====\n",
    "grid_rows = 1  # Number of spots along Y\n",
    "grid_cols = 3  # Number of spots along X\n",
    "sigma = 0.001   # Standard deviation in normalized coordinates [-1, 1]\n",
    "spacing_mode = 'uniform'  # 'uniform' or 'random'\n",
    "random_seed = 42  # Only used if spacing_mode='random'\n",
    "base_intensity = 100000.0  # Base intensity (will be divided by num_spots)\n",
    "base_spp = 10000  # Base samples per pixel (will be multiplied by num_spots)\n",
    "max_rejection_samples = 4  # Max rejection sampling iterations for out-of-FOV samples\n",
    "fov = 0.55\n",
    "\n",
    "num_spots = grid_rows * grid_cols\n",
    "intensity = base_intensity / num_spots  # Divide intensity by number of spots\n",
    "spp = base_spp * num_spots  # Scale samples by number of spots\n",
    "\n",
    "print(f\"Parametric projector configuration:\")\n",
    "print(f\"  Grid size: {grid_rows} rows x {grid_cols} cols = {num_spots} spots\")\n",
    "print(f\"  Gaussian sigma (normalized): {sigma}\")\n",
    "print(f\"  Intensity per spot: {intensity} (base: {base_intensity})\")\n",
    "print(f\"  Samples per pixel: {spp} (base: {base_spp})\")\n",
    "print(f\"  Max rejection samples: {max_rejection_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Remove keys from a list\nkeys_to_remove = ['light', 'integrator']  # Example: remove these plugins if present\nfor k in keys_to_remove:\n    d.pop(k, None)\n\n# Update film parameters for batch rendering\nd['sensor']['film']['temporal_bins'] = 1000\nd['sensor']['film']['width'] = W\nd['sensor']['film']['height'] = H\nprint(d['sensor'])\n\n# Get camera to_world transform directly from the sensor\n# This is already a Transform4f with rotation and position\ncamera_to_world = d['sensor']['to_world']\n\nprint(f\"\\nCamera to_world transform:\")\nprint(camera_to_world)\n\n# Create the ConfocalProjector emitter using grid mode\n# Pass the camera transform as the projector frame for non-confocal mode\nfrom mitransient.emitters.confocal_projector import ConfocalProjector\n\nprojector = mi.load_dict({\n    \"type\": \"confocal_projector\",\n    \"grid_rows\": grid_rows,\n    \"grid_cols\": grid_cols,\n    \"grid_sigma\": sigma,\n    \"grid_intensity\": intensity,\n    \"grid_spacing\": spacing_mode,\n    \"fov\": fov,\n    \"is_confocal\": False,  # Set to False to use static projector frame\n    \"frame\": camera_to_world,  # Pass full 4x4 Transform4f (rotation + position)\n    \"max_rejection_samples\": max_rejection_samples,\n})\n\nprint(f\"\\nConfocal projector created:\")\nprint(projector.to_string())\n\n# Setup integrator with reference to the projector\nintegrator = mi.load_dict({\n    \"type\": \"transient_path\",\n    \"temporal_filter\": \"gaussian\",\n    \"gaussian_stddev\": 4.0,\n    \"use_nlos_only\": False,\n    \"camera_unwarp\": True,\n    \"confocal_projector\": projector,\n})\nd['integrator'] = integrator\n\nprint(f\"\\nScene created successfully!\")\nprint(f\"Using parametric projector: {grid_rows}x{grid_cols} grid, sigma={sigma}\")\nprint(f\"Number of spots: {num_spots}\")\nprint(f\"Samples per pixel: {spp}\")\nprint(f\"Projector mode: {'Confocal (dynamic)' if projector.is_confocal else 'Static (camera-aligned)'}\")\n\nscene = mi.load_dict(d)\ndata_steady, data_transient = mi.render(scene, spp=spp)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.103059374\n"
     ]
    }
   ],
   "source": [
    "# Plot the computed transient image as a video\n",
    "data_transient_clipped = dr.clip(data_transient, 0.0, 1.0)\n",
    "data_transient_tonemapped = mitr.vis.tonemap_transient(data_transient_clipped)\n",
    "\n",
    "mitr.vis.save_video(\n",
    "    'results/cornell_confocal_output.mp4',\n",
    "    data_transient_tonemapped,\n",
    "    axis_video=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx-thumbnail": {}
   },
   "outputs": [],
   "source": [
    "# Plot some frames of the computed transient image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_transient_tonemapped[data_transient_tonemapped > 1] = 1\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(data_transient_tonemapped[:, :, 100])  # frame 100 (video has 300 frames)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(data_transient_tonemapped[:, :, 140])  # frame 140 (video has 300 frames)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tlir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}