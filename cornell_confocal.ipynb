{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scalar_rgb', 'scalar_spectral', 'scalar_spectral_polarized', 'llvm_ad_rgb', 'llvm_ad_mono', 'llvm_ad_mono_polarized', 'llvm_ad_spectral', 'llvm_ad_spectral_polarized', 'cuda_ad_rgb', 'cuda_ad_mono', 'cuda_ad_mono_polarized', 'cuda_ad_spectral', 'cuda_ad_spectral_polarized']\n",
      "Using mitsuba version: 3.7.0\n",
      "Using mitransient version: 1.2.0\n"
     ]
    }
   ],
   "source": [
    "# If you have compiled Mitsuba 3 yourself, you will need to specify the path\n",
    "# to the compilation folder\n",
    "# import sys\n",
    "# sys.path.insert(0, '<mitsuba-path>/mitsuba3/build/python')\n",
    "import mitsuba as mi\n",
    "# To set a variant, you need to have set it in the mitsuba.conf file\n",
    "# https://mitsuba.readthedocs.io/en/latest/src/key_topics/variants.html\n",
    "print(mi.variants())\n",
    "mi.set_variant('cuda_ad_rgb')\n",
    "\n",
    "import mitransient as mitr\n",
    "\n",
    "print('Using mitsuba version:', mi.__version__)\n",
    "print('Using mitransient version:', mitr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import drjit as dr\n",
    "import numpy as np\n",
    "\n",
    "def get_rays_mitsuba(H, W, fx, fy, cx, cy, c2w_matrix):\n",
    "    \"\"\"\n",
    "    Generate camera rays using DrJit/Mitsuba instead of PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        H, W: Image height and width\n",
    "        fx, fy: Focal lengths\n",
    "        cx, cy: Principal point coordinates\n",
    "        c2w_matrix: Camera-to-world transformation matrix (4x4 numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        origins: Ray origins (H, W, 3) numpy array\n",
    "        viewdirs: Ray directions (H, W, 3) numpy array\n",
    "    \"\"\"\n",
    "    # Create pixel coordinates\n",
    "    num_pixels = H * W\n",
    "    idx = np.arange(num_pixels)\n",
    "    x = idx % W\n",
    "    y = idx // W\n",
    "    \n",
    "    # Convert to image coordinates with pixel centers\n",
    "    # Using the same convention as the torch code: (x - cx + 0.5) / fx\n",
    "    pixel_x = (x - cx + 0.5) / fx\n",
    "    pixel_y = (y - cy + 0.5) / fy * -1.0  # Flip Y axis\n",
    "    \n",
    "    # Create direction vectors in camera space\n",
    "    # Format: [pixel_x, pixel_y, -1.0] (pointing into the scene)\n",
    "    dirs_camera = np.stack([pixel_x, pixel_y, np.full(num_pixels, -1.0)], axis=-1)\n",
    "    \n",
    "    # Extract rotation and translation from c2w matrix\n",
    "    rotation = c2w_matrix[:3, :3]\n",
    "    translation = c2w_matrix[:3, 3]\n",
    "    \n",
    "    # Transform directions to world space\n",
    "    directions = dirs_camera @ rotation.T  # (num_pixels, 3) @ (3, 3).T\n",
    "    \n",
    "    # Normalize directions\n",
    "    directions = directions / np.linalg.norm(directions, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Broadcast camera origin to all rays\n",
    "    origins = np.broadcast_to(translation, (num_pixels, 3))\n",
    "    \n",
    "    # Reshape to image dimensions\n",
    "    origins = origins.reshape(H, W, 3)\n",
    "    viewdirs = directions.reshape(H, W, 3)\n",
    "    \n",
    "    # Negate viewdirs to match torch convention\n",
    "    viewdirs = -1 * viewdirs\n",
    "    \n",
    "    return origins, viewdirs\n",
    "\n",
    "d = mitr.cornell_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origins shape: (255, 255, 3)\n",
      "Rays shape: (255, 255, 3)\n",
      "Sample ray direction at (0,0): [-0.31976103 -0.31976101 -0.89191131]\n",
      "Sample ray origin at (0,0): [0.  0.  3.9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Image dimensions\n",
    "H, W = 255, 255\n",
    "fov = 0.69097585\n",
    "\n",
    "# Compute camera intrinsics\n",
    "fx = (W / 2.0) / np.tan(fov / 2.0)\n",
    "fy = fx\n",
    "cx = float(W) / 2.0\n",
    "cy = float(H) / 2.0\n",
    "\n",
    "# Get camera-to-world matrix from the sensor\n",
    "c2w_transform = d['sensor']['to_world']\n",
    "c2w_matrix = np.array(c2w_transform.matrix)\n",
    "\n",
    "# Generate rays using Mitsuba/DrJit approach\n",
    "origins, rays = get_rays_mitsuba(H, W, fx, fy, cx, cy, c2w_matrix)\n",
    "\n",
    "print(f\"Origins shape: {origins.shape}\")\n",
    "print(f\"Rays shape: {rays.shape}\")\n",
    "print(f\"Sample ray direction at (0,0): {rays[0, 0]}\")\n",
    "print(f\"Sample ray origin at (0,0): {origins[0, 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "projector-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian pattern created: (64, 64, 3)\n",
      "Center pixel value: 0.039788734167814255\n",
      "Pattern sum (should be ~1.0): 1.0\n",
      "Bitmap created: resolution [64, 64]\n"
     ]
    }
   ],
   "source": [
    "# Create Gaussian pattern for projector\n",
    "import numpy as np\n",
    "\n",
    "def create_gaussian_pattern(width, height, center_x, center_y, sigma):\n",
    "    \"\"\"Create a Gaussian pattern centered at (center_x, center_y).\"\"\"\n",
    "    x = np.arange(width)\n",
    "    y = np.arange(height)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    gaussian = np.exp(-((xx - center_x)**2 + (yy - center_y)**2) / (2 * sigma**2))\n",
    "    gaussian = gaussian / np.sum(gaussian)\n",
    "    gaussian_rgb = np.stack([gaussian, gaussian, gaussian], axis=-1)\n",
    "    return gaussian_rgb.astype(np.float32)\n",
    "\n",
    "# Create projector pattern: small-width Gaussian centered at center-most pixel\n",
    "proj_width, proj_height = 64, 64\n",
    "center_x, center_y = proj_width // 2, proj_height // 2\n",
    "sigma = 2.0  # Small width Gaussian\n",
    "\n",
    "pattern = create_gaussian_pattern(proj_width, proj_height, center_x, center_y, sigma)\n",
    "print(f\"Gaussian pattern created: {pattern.shape}\")\n",
    "print(f\"Center pixel value: {pattern[center_y, center_x, 0]}\")\n",
    "print(f\"Pattern sum (should be ~1.0): {np.sum(pattern[:,:,0])}\")\n",
    "\n",
    "# Create bitmap from pattern\n",
    "bitmap = mi.Bitmap(pattern)\n",
    "print(f\"Bitmap created: resolution {bitmap.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'perspective', 'fov_axis': 'smaller', 'near_clip': 0.001, 'far_clip': 100.0, 'focus_distance': 1000, 'fov': 39.3077, 'to_world': Transform[\n",
      "  matrix=[[-1, 0, 0, 0],\n",
      "          [0, 1, 0, 0],\n",
      "          [0, 0, -1, 3.9],\n",
      "          [0, 0, 0, 1]],\n",
      "  inverse_transpose=[[-1, 0, 0, 0],\n",
      "                     [0, 1, 0, 0],\n",
      "                     [0, 0, -1, 0],\n",
      "                     [0, 0, 3.9, 1]]\n",
      "], 'sampler': {'type': 'independent', 'sample_count': 256}, 'film': {'type': 'transient_hdr_film', 'width': 255, 'height': 255, 'rfilter': {'type': 'box'}, 'temporal_bins': 1000, 'start_opl': 3.5, 'bin_width_opl': 0.02}}\n",
      "\n",
      "Scene created successfully!\n",
      "Light origin: [0.  0.  3.9]\n",
      "Light target: [ 0.06474757 -0.03659645  2.9027697 ]\n",
      "Using Gaussian projector pattern with sigma=2.0\n"
     ]
    }
   ],
   "source": [
    "# Remove keys from a list\n",
    "keys_to_remove = ['light', 'integrator']  # Example: remove these plugins if present\n",
    "for k in keys_to_remove:\n",
    "    d.pop(k, None)\n",
    "\n",
    "# Update film parameters for batch rendering\n",
    "d['sensor']['film']['temporal_bins'] = 1000\n",
    "d['sensor']['film']['width'] = W\n",
    "d['sensor']['film']['height'] = H\n",
    "print(d['sensor'])\n",
    "\n",
    "# Setup the projector light source with Gaussian pattern\n",
    "pointx, pointy = (140, 150)\n",
    "origin = origins[254 - pointx, pointy]\n",
    "target = rays[254 - pointx, pointy] + origin\n",
    "\n",
    "emitter = mi.load_dict({\n",
    "    \"type\": \"projector\",\n",
    "    \"irradiance\": {\n",
    "        \"type\": \"bitmap\",\n",
    "        \"bitmap\": bitmap,  # Use the Gaussian pattern bitmap we created\n",
    "    },\n",
    "    \"fov\": 2e-1,\n",
    "    \"to_world\": mi.ScalarTransform4f().look_at(\n",
    "        origin=list(origin),  # Convert numpy array to list\n",
    "        target=list(target),  # Convert numpy array to list\n",
    "        up=[0, 1, 0]),\n",
    "})\n",
    "\n",
    "d['light'] = emitter\n",
    "\n",
    "# Setup integrator\n",
    "integrator = mi.load_dict(\n",
    "    {\n",
    "        \"type\": \"transient_path\",\n",
    "        \"temporal_filter\": \"box\",\n",
    "        \"use_confocal_light_source\": True,\n",
    "        \"use_nlos_only\": False,\n",
    "    }\n",
    ")\n",
    "d['integrator'] = integrator\n",
    "\n",
    "print(f\"\\nScene created successfully!\")\n",
    "print(f\"Light origin: {origin}\")\n",
    "print(f\"Light target: {target}\")\n",
    "print(f\"Using Gaussian projector pattern with sigma={sigma}\")\n",
    "\n",
    "scene = mi.load_dict(d)\n",
    "data_steady, data_transient = mi.render(scene, spp=10240)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the computed transient image as a video\n",
    "data_transient_tonemapped = mitr.vis.tonemap_transient(data_transient)\n",
    "\n",
    "mitr.vis.save_video(\n",
    "    'stupid.mp4',\n",
    "    data_transient_tonemapped,\n",
    "    axis_video=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx-thumbnail": {}
   },
   "outputs": [],
   "source": [
    "# Plot some frames of the computed transient image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_transient_tonemapped[data_transient_tonemapped > 1] = 1\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(data_transient_tonemapped[:, :, 100])  # frame 100 (video has 300 frames)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(data_transient_tonemapped[:, :, 140])  # frame 140 (video has 300 frames)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tlir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
